{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal is to write GRPO from scratch and train on maths dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tf-keras\n",
      "  Downloading tf_keras-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting tensorflow<2.20,>=2.19 (from tf-keras)\n",
      "  Downloading tensorflow-2.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow<2.20,>=2.19->tf-keras)\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow<2.20,>=2.19->tf-keras)\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow<2.20,>=2.19->tf-keras)\n",
      "  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow<2.20,>=2.19->tf-keras)\n",
      "  Downloading gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow<2.20,>=2.19->tf-keras)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow<2.20,>=2.19->tf-keras)\n",
      "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow<2.20,>=2.19->tf-keras)\n",
      "  Downloading opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging in /home/ubuntu/grpo/lib/python3.11/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (24.2)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 (from tensorflow<2.20,>=2.19->tf-keras)\n",
      "  Downloading protobuf-5.29.3-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Collecting requests<3,>=2.21.0 (from tensorflow<2.20,>=2.19->tf-keras)\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting setuptools (from tensorflow<2.20,>=2.19->tf-keras)\n",
      "  Downloading setuptools-76.0.0-py3-none-any.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/ubuntu/grpo/lib/python3.11/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.17.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow<2.20,>=2.19->tf-keras)\n",
      "  Downloading termcolor-2.5.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/ubuntu/grpo/lib/python3.11/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (4.12.2)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow<2.20,>=2.19->tf-keras)\n",
      "  Downloading wrapt-1.17.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow<2.20,>=2.19->tf-keras)\n",
      "  Downloading grpcio-1.71.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tensorboard~=2.19.0 (from tensorflow<2.20,>=2.19->tf-keras)\n",
      "  Downloading tensorboard-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting keras>=3.5.0 (from tensorflow<2.20,>=2.19->tf-keras)\n",
      "  Downloading keras-3.9.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting numpy<2.2.0,>=1.26.0 (from tensorflow<2.20,>=2.19->tf-keras)\n",
      "  Downloading numpy-2.1.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "Collecting h5py>=3.11.0 (from tensorflow<2.20,>=2.19->tf-keras)\n",
      "  Downloading h5py-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
      "Collecting ml-dtypes<1.0.0,>=0.5.1 (from tensorflow<2.20,>=2.19->tf-keras)\n",
      "  Downloading ml_dtypes-0.5.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow<2.20,>=2.19->tf-keras)\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.37.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
      "Collecting wheel<1.0,>=0.23.0 (from astunparse>=1.6.0->tensorflow<2.20,>=2.19->tf-keras)\n",
      "  Downloading wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting rich (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras)\n",
      "  Downloading rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting namex (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras)\n",
      "  Downloading namex-0.0.8-py3-none-any.whl.metadata (246 bytes)\n",
      "Collecting optree (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras)\n",
      "  Downloading optree-0.14.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (49 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras)\n",
      "  Downloading charset_normalizer-3.4.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (35 kB)\n",
      "Collecting idna<4,>=2.5 (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras)\n",
      "  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras)\n",
      "  Downloading urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras)\n",
      "  Downloading certifi-2025.1.31-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras)\n",
      "  Downloading Markdown-3.7-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras)\n",
      "  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting MarkupSafe>=2.1.1 (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras)\n",
      "  Downloading MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/ubuntu/grpo/lib/python3.11/site-packages (from rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (2.19.1)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading tf_keras-2.19.0-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m44.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow-2.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (644.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m644.9/644.9 MB\u001b[0m \u001b[31m122.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
      "Downloading gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Downloading grpcio-1.71.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m160.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading h5py-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m139.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading keras-3.9.0-py3-none-any.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m118.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m178.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ml_dtypes-0.5.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m157.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-2.1.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m144.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Downloading protobuf-5.29.3-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
      "Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Downloading tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m139.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading setuptools-76.0.0-py3-none-any.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m76.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow_io_gcs_filesystem-0.37.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m143.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading termcolor-2.5.0-py3-none-any.whl (7.8 kB)\n",
      "Downloading wrapt-1.17.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (83 kB)\n",
      "Downloading certifi-2025.1.31-py3-none-any.whl (166 kB)\n",
      "Downloading charset_normalizer-3.4.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (143 kB)\n",
      "Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
      "Downloading Markdown-3.7-py3-none-any.whl (106 kB)\n",
      "Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m153.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
      "Downloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "Downloading wheel-0.45.1-py3-none-any.whl (72 kB)\n",
      "Downloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
      "Downloading optree-0.14.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (408 kB)\n",
      "Downloading rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: namex, libclang, flatbuffers, wrapt, wheel, urllib3, termcolor, tensorflow-io-gcs-filesystem, tensorboard-data-server, setuptools, protobuf, optree, opt-einsum, numpy, mdurl, MarkupSafe, markdown, idna, grpcio, google-pasta, gast, charset-normalizer, certifi, absl-py, werkzeug, requests, ml-dtypes, markdown-it-py, h5py, astunparse, tensorboard, rich, keras, tensorflow, tf-keras\n",
      "Successfully installed MarkupSafe-3.0.2 absl-py-2.1.0 astunparse-1.6.3 certifi-2025.1.31 charset-normalizer-3.4.1 flatbuffers-25.2.10 gast-0.6.0 google-pasta-0.2.0 grpcio-1.71.0 h5py-3.13.0 idna-3.10 keras-3.9.0 libclang-18.1.1 markdown-3.7 markdown-it-py-3.0.0 mdurl-0.1.2 ml-dtypes-0.5.1 namex-0.0.8 numpy-2.1.3 opt-einsum-3.4.0 optree-0.14.1 protobuf-5.29.3 requests-2.32.3 rich-13.9.4 setuptools-76.0.0 tensorboard-2.19.0 tensorboard-data-server-0.7.2 tensorflow-2.19.0 tensorflow-io-gcs-filesystem-0.37.1 termcolor-2.5.0 tf-keras-2.19.0 urllib3-2.3.0 werkzeug-3.1.3 wheel-0.45.1 wrapt-1.17.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting flash-attn\n",
      "  Downloading flash_attn-2.7.4.post1.tar.gz (6.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m93.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[6 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \"<string>\", line 2, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"<pip-setuptools-caller>\", line 34, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-install-jgm44vb9/flash-attn_32fd1f9a6cdf4ef1a9631995a8265d31/setup.py\", line 22, in <module>\n",
      "  \u001b[31m   \u001b[0m     import torch\n",
      "  \u001b[31m   \u001b[0m ModuleNotFoundError: No module named 'torch'\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[?25h\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
      "\u001b[1;36mhint\u001b[0m: See above for details.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting wandb\n",
      "  Downloading wandb-0.19.8-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Collecting click!=8.0.0,>=7.1 (from wandb)\n",
      "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting docker-pycreds>=0.4.0 (from wandb)\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb)\n",
      "  Downloading GitPython-3.1.44-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: platformdirs in /home/ubuntu/grpo/lib/python3.11/site-packages (from wandb) (4.3.6)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /home/ubuntu/grpo/lib/python3.11/site-packages (from wandb) (5.29.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /home/ubuntu/grpo/lib/python3.11/site-packages (from wandb) (7.0.0)\n",
      "Collecting pydantic<3,>=2.6 (from wandb)\n",
      "  Downloading pydantic-2.10.6-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting pyyaml (from wandb)\n",
      "  Downloading PyYAML-6.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /home/ubuntu/grpo/lib/python3.11/site-packages (from wandb) (2.32.3)\n",
      "Collecting sentry-sdk>=2.0.0 (from wandb)\n",
      "  Downloading sentry_sdk-2.22.0-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Collecting setproctitle (from wandb)\n",
      "  Downloading setproctitle-1.3.5-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: setuptools in /home/ubuntu/grpo/lib/python3.11/site-packages (from wandb) (76.0.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4 in /home/ubuntu/grpo/lib/python3.11/site-packages (from wandb) (4.12.2)\n",
      "Requirement already satisfied: six>=1.4.0 in /home/ubuntu/grpo/lib/python3.11/site-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb)\n",
      "  Downloading gitdb-4.0.12-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3,>=2.6->wandb)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.27.2 (from pydantic<3,>=2.6->wandb)\n",
      "  Downloading pydantic_core-2.27.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ubuntu/grpo/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ubuntu/grpo/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ubuntu/grpo/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/grpo/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb) (2025.1.31)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb)\n",
      "  Downloading smmap-5.0.2-py3-none-any.whl.metadata (4.3 kB)\n",
      "Downloading wandb-0.19.8-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.8/20.8 MB\u001b[0m \u001b[31m143.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Downloading GitPython-3.1.44-py3-none-any.whl (207 kB)\n",
      "Downloading pydantic-2.10.6-py3-none-any.whl (431 kB)\n",
      "Downloading pydantic_core-2.27.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m140.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sentry_sdk-2.22.0-py2.py3-none-any.whl (325 kB)\n",
      "Downloading PyYAML-6.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (762 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m763.0/763.0 kB\u001b[0m \u001b[31m79.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading setproctitle-1.3.5-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading gitdb-4.0.12-py3-none-any.whl (62 kB)\n",
      "Downloading smmap-5.0.2-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: smmap, setproctitle, sentry-sdk, pyyaml, pydantic-core, docker-pycreds, click, annotated-types, pydantic, gitdb, gitpython, wandb\n",
      "Successfully installed annotated-types-0.7.0 click-8.1.8 docker-pycreds-0.4.0 gitdb-4.0.12 gitpython-3.1.44 pydantic-2.10.6 pydantic-core-2.27.2 pyyaml-6.0.2 sentry-sdk-2.22.0 setproctitle-1.3.5 smmap-5.0.2 wandb-0.19.8\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting accelerate>=0.26.0\n",
      "  Downloading accelerate-1.5.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /home/ubuntu/grpo/lib/python3.11/site-packages (from accelerate>=0.26.0) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ubuntu/grpo/lib/python3.11/site-packages (from accelerate>=0.26.0) (24.2)\n",
      "Requirement already satisfied: psutil in /home/ubuntu/grpo/lib/python3.11/site-packages (from accelerate>=0.26.0) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in /home/ubuntu/grpo/lib/python3.11/site-packages (from accelerate>=0.26.0) (6.0.2)\n",
      "Collecting torch>=2.0.0 (from accelerate>=0.26.0)\n",
      "  Downloading torch-2.6.0-cp311-cp311-manylinux1_x86_64.whl.metadata (28 kB)\n",
      "Collecting huggingface_hub>=0.21.0 (from accelerate>=0.26.0)\n",
      "  Downloading huggingface_hub-0.29.3-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting safetensors>=0.4.3 (from accelerate>=0.26.0)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting filelock (from huggingface_hub>=0.21.0->accelerate>=0.26.0)\n",
      "  Downloading filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface_hub>=0.21.0->accelerate>=0.26.0)\n",
      "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: requests in /home/ubuntu/grpo/lib/python3.11/site-packages (from huggingface_hub>=0.21.0->accelerate>=0.26.0) (2.32.3)\n",
      "Collecting tqdm>=4.42.1 (from huggingface_hub>=0.21.0->accelerate>=0.26.0)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ubuntu/grpo/lib/python3.11/site-packages (from huggingface_hub>=0.21.0->accelerate>=0.26.0) (4.12.2)\n",
      "Collecting networkx (from torch>=2.0.0->accelerate>=0.26.0)\n",
      "  Downloading networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jinja2 (from torch>=2.0.0->accelerate>=0.26.0)\n",
      "  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.0.0->accelerate>=0.26.0)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.0.0->accelerate>=0.26.0)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.0.0->accelerate>=0.26.0)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.0->accelerate>=0.26.0)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0.0->accelerate>=0.26.0)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0.0->accelerate>=0.26.0)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0.0->accelerate>=0.26.0)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0.0->accelerate>=0.26.0)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0.0->accelerate>=0.26.0)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.6.2 (from torch>=2.0.0->accelerate>=0.26.0)\n",
      "  Downloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting nvidia-nccl-cu12==2.21.5 (from torch>=2.0.0->accelerate>=0.26.0)\n",
      "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.4.127 (from torch>=2.0.0->accelerate>=0.26.0)\n",
      "  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0.0->accelerate>=0.26.0)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting triton==3.2.0 (from torch>=2.0.0->accelerate>=0.26.0)\n",
      "  Downloading triton-3.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
      "Collecting sympy==1.13.1 (from torch>=2.0.0->accelerate>=0.26.0)\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch>=2.0.0->accelerate>=0.26.0)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ubuntu/grpo/lib/python3.11/site-packages (from jinja2->torch>=2.0.0->accelerate>=0.26.0) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ubuntu/grpo/lib/python3.11/site-packages (from requests->huggingface_hub>=0.21.0->accelerate>=0.26.0) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ubuntu/grpo/lib/python3.11/site-packages (from requests->huggingface_hub>=0.21.0->accelerate>=0.26.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ubuntu/grpo/lib/python3.11/site-packages (from requests->huggingface_hub>=0.21.0->accelerate>=0.26.0) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/grpo/lib/python3.11/site-packages (from requests->huggingface_hub>=0.21.0->accelerate>=0.26.0) (2025.1.31)\n",
      "Downloading accelerate-1.5.1-py3-none-any.whl (345 kB)\n",
      "Downloading huggingface_hub-0.29.3-py3-none-any.whl (468 kB)\n",
      "Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "Downloading torch-2.6.0-cp311-cp311-manylinux1_x86_64.whl (766.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m766.7/766.7 MB\u001b[0m \u001b[31m108.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m157.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m162.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m166.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m86.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m122.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m171.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m181.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m183.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m186.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl (150.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m188.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m190.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m181.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
      "Downloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m164.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading triton-3.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (253.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.2/253.2 MB\u001b[0m \u001b[31m154.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Downloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m132.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m54.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: triton, nvidia-cusparselt-cu12, mpmath, tqdm, sympy, safetensors, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, networkx, jinja2, fsspec, filelock, nvidia-cusparse-cu12, nvidia-cudnn-cu12, huggingface_hub, nvidia-cusolver-cu12, torch, accelerate\n",
      "Successfully installed accelerate-1.5.1 filelock-3.18.0 fsspec-2025.3.0 huggingface_hub-0.29.3 jinja2-3.1.6 mpmath-1.3.0 networkx-3.4.2 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-cusparselt-cu12-0.6.2 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 safetensors-0.5.3 sympy-1.13.1 torch-2.6.0 tqdm-4.67.1 triton-3.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.49.0-py3-none-any.whl.metadata (44 kB)\n",
      "Requirement already satisfied: filelock in /home/ubuntu/grpo/lib/python3.11/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /home/ubuntu/grpo/lib/python3.11/site-packages (from transformers) (0.29.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ubuntu/grpo/lib/python3.11/site-packages (from transformers) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ubuntu/grpo/lib/python3.11/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ubuntu/grpo/lib/python3.11/site-packages (from transformers) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2024.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /home/ubuntu/grpo/lib/python3.11/site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/ubuntu/grpo/lib/python3.11/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ubuntu/grpo/lib/python3.11/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ubuntu/grpo/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ubuntu/grpo/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ubuntu/grpo/lib/python3.11/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ubuntu/grpo/lib/python3.11/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ubuntu/grpo/lib/python3.11/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/grpo/lib/python3.11/site-packages (from requests->transformers) (2025.1.31)\n",
      "Downloading transformers-4.49.0-py3-none-any.whl (10.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (792 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m792.7/792.7 kB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m51.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: regex, tokenizers, transformers\n",
      "Successfully installed regex-2024.11.6 tokenizers-0.21.1 transformers-4.49.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting datasets\n",
      "  Downloading datasets-3.3.2-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in /home/ubuntu/grpo/lib/python3.11/site-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ubuntu/grpo/lib/python3.11/site-packages (from datasets) (2.1.3)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Downloading pyarrow-19.0.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets)\n",
      "  Downloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/ubuntu/grpo/lib/python3.11/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/ubuntu/grpo/lib/python3.11/site-packages (from datasets) (4.67.1)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n",
      "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Downloading aiohttp-3.11.13-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /home/ubuntu/grpo/lib/python3.11/site-packages (from datasets) (0.29.3)\n",
      "Requirement already satisfied: packaging in /home/ubuntu/grpo/lib/python3.11/site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ubuntu/grpo/lib/python3.11/site-packages (from datasets) (6.0.2)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp->datasets)\n",
      "  Downloading attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Downloading frozenlist-1.5.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Downloading multidict-6.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp->datasets)\n",
      "  Downloading propcache-0.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->datasets)\n",
      "  Downloading yarl-1.18.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (69 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ubuntu/grpo/lib/python3.11/site-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ubuntu/grpo/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ubuntu/grpo/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ubuntu/grpo/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/grpo/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ubuntu/grpo/lib/python3.11/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets)\n",
      "  Downloading pytz-2025.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets)\n",
      "  Downloading tzdata-2025.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /home/ubuntu/grpo/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Downloading datasets-3.3.2-py3-none-any.whl (485 kB)\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
      "Downloading aiohttp-3.11.13-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m76.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "Downloading pyarrow-19.0.1-cp311-cp311-manylinux_2_28_x86_64.whl (42.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.1/42.1 MB\u001b[0m \u001b[31m127.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m121.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading attrs-25.3.0-py3-none-any.whl (63 kB)\n",
      "Downloading frozenlist-1.5.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (274 kB)\n",
      "Downloading multidict-6.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
      "Downloading propcache-0.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (231 kB)\n",
      "Downloading pytz-2025.1-py2.py3-none-any.whl (507 kB)\n",
      "Downloading tzdata-2025.1-py2.py3-none-any.whl (346 kB)\n",
      "Downloading yarl-1.18.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (344 kB)\n",
      "Installing collected packages: pytz, xxhash, tzdata, pyarrow, propcache, multidict, fsspec, frozenlist, dill, attrs, aiohappyeyeballs, yarl, pandas, multiprocess, aiosignal, aiohttp, datasets\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.3.0\n",
      "    Uninstalling fsspec-2025.3.0:\n",
      "      Successfully uninstalled fsspec-2025.3.0\n",
      "Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.11.13 aiosignal-1.3.2 attrs-25.3.0 datasets-3.3.2 dill-0.3.8 frozenlist-1.5.0 fsspec-2024.12.0 multidict-6.1.0 multiprocess-0.70.16 pandas-2.2.3 propcache-0.3.0 pyarrow-19.0.1 pytz-2025.1 tzdata-2025.1 xxhash-3.5.0 yarl-1.18.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tf-keras # for some reason, Hugging Face cannot work without it\n",
    "%pip install flash-attn # FlashAttention2\n",
    "%pip install wandb # Weights and Biases\n",
    "%pip install 'accelerate>=0.26.0'\n",
    "%pip install transformers # Hugging Face Transformers API\n",
    "%pip install datasets # Hugging Face Datasets API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/grpo/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/ubuntu/grpo/lib/python3.11/site-packages/_distutils_hack/__init__.py:15: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/grpo/lib/python3.11/site-packages/_distutils_hack/__init__.py:30: UserWarning: Setuptools is replacing distutils. Support for replacing an already imported distutils is deprecated. In the future, this condition will fail. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "# Basic Python libraries for various operations\n",
    "import random\n",
    "import copy\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import wandb\n",
    "\n",
    "# PyTorch and related libraries for deep learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Hugging Face libraries for transformer models\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_random_seed(seed: int = 42):\n",
    "    # Set the seed for Python's built-in random module\n",
    "    random.seed(seed)\n",
    "    # Set the seed for NumPy\n",
    "    np.random.seed(seed)\n",
    "    # Set the seed for PyTorch\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    # Ensure deterministic behavior in cuDNN (may impact performance)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "set_random_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set environment variables for Weights & Biases (wandb) logging\n",
    "os.environ[\"WANDB_API_KEY\"] = \"3217ad321534a113446df07670c404af4d018fd0\"\n",
    "os.environ[\"WANDB_PROJECT\"] = \"GRPO-Qwen-1.5-Instruct-Multi-GPU\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model generates output in an XML-like format with <reasoning> and <answer> tags.\n",
    "# Two functions are provided:\n",
    "# 1. extract_answer_from_model_output: Extracts content within <answer> tags.\n",
    "# 2. extract_answer_from_dataset: Extracts the expected answer from the GSM8K dataset using \"####\" as a delimiter.\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "Respond in the following format:\n",
    "<reasoning>\n",
    "...\n",
    "</reasoning>\n",
    "<answer>\n",
    "...\n",
    "</answer>\n",
    "\"\"\"\n",
    "\n",
    "def extract_answer_from_model_output(text):\n",
    "    # Extract the answer from the model's output\n",
    "    parts = text.split(\"<answer>\")\n",
    "    if len(parts) < 2:\n",
    "        return None\n",
    "    last_part = parts[-1]\n",
    "\n",
    "    if \"</answer>\" not in last_part:\n",
    "        return None\n",
    "    answer = last_part.split(\"</answer>\")[0].strip()\n",
    "    return None if answer == \"...\" else answer\n",
    "\n",
    "def extract_answer_from_dataset(text):\n",
    "    if \"####\" not in text:\n",
    "        return None\n",
    "    return text.split(\"####\")[1].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the dataset for training\n",
    "def prepare_dataset(split=\"train\"):\n",
    "    # Load the GSM8K dataset\n",
    "    data = load_dataset('openai/gsm8k', 'main')[split]\n",
    "    formatted_data = []\n",
    "    for example in data:\n",
    "        prompt_str = build_prompt([\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": example[\"question\"]},\n",
    "        ])\n",
    "        formatted_example = {\n",
    "            \"prompt\": prompt_str,\n",
    "            \"answer\": extract_answer_from_dataset(example[\"answer\"]),\n",
    "        }\n",
    "        formatted_data.append(formatted_example)\n",
    "    return formatted_data\n",
    "\n",
    "\n",
    "def build_prompt(messages):\n",
    "    # Build the prompt for the model\n",
    "    return \"\\n\".join([m[\"content\"].strip() for m in messages])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_last_number(text):\n",
    "    text = text.replace('$', '').replace('%', '')\n",
    "    pattern = r'(?:^|\\s|=)\\s*(-?\\d*\\.?\\d+)\\s*$'\n",
    "    match = re.search(pattern, text)\n",
    "    return float(match.group(1)) if match else None\n",
    "\n",
    "\n",
    "def extract_single_number(text):\n",
    "    numbers = re.findall(r'-?\\d*\\.?\\d+', text)\n",
    "    return float(numbers[0]) if len(numbers) == 1 else None\n",
    "\n",
    "\"\"\"\n",
    "Explanation:\n",
    "       1. Sets the model to evaluation mode.\n",
    "       2. For each example in the evaluation set:\n",
    "          - Encodes the prompt and generates a response using the model.\n",
    "          - Extracts the predicted answer from the generated response.\n",
    "          - Compares the predicted answer with the expected answer using multiple methods:\n",
    "            a. Exact string matching\n",
    "            b. Single number extraction and comparison\n",
    "            c. Last number extraction and comparison\n",
    "          - Prints detailed information about each example.\n",
    "       3. Calculates and returns the overall accuracy.\n",
    "       4. Returns the model to training mode.\n",
    "\"\"\"\n",
    "def evaluate_model(model, tokenizer, eval_examples, device):\n",
    "    model.eval() # Set the model to evaluation mode\n",
    "    correct = 0 # Initialize the number of correct predictions\n",
    "    total = len(eval_examples) # Get the total number of examples\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Evaluation on {} examples\".format(total))\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    for example in eval_examples:\n",
    "        # Get the full prompt and expected answer\n",
    "        full_prompt = example[\"prompt\"]\n",
    "        expected = example[\"answer\"]\n",
    "\n",
    "        # Tokenize and generate response\n",
    "        inputs = tokenizer.encode(full_prompt, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                inputs,\n",
    "                max_new_tokens=512, # Maximum number of tokens to generate\n",
    "                temperature=0.7, # Sampling temperature\n",
    "                num_return_sequences=1, # Number of sequences to generate\n",
    "                pad_token_id=tokenizer.pad_token_id, # Padding token ID\n",
    "                eos_token_id=tokenizer.eos_token_id, # End-of-sequence token ID\n",
    "                forced_eos_token_id=tokenizer.eos_token_id, # Force the model to stop at the end-of-sequence token\n",
    "                early_stopping=False, # Do not stop early\n",
    "            )\n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "        try:\n",
    "            # Extract the answer and check for correctness\n",
    "            predicted = extract_answer_from_model_output(response)\n",
    "\n",
    "            # Try different methods for comparison\n",
    "            if predicted == expected:\n",
    "                is_correct = True\n",
    "            else:\n",
    "                # Try single number matching\n",
    "                pred_num = extract_single_number(str(predicted))\n",
    "                exp_num = extract_single_number(str(expected))\n",
    "                if pred_num is not None and exp_num is not None and pred_num == exp_num:\n",
    "                    is_correct = True\n",
    "                else:\n",
    "                    # Try last number matching\n",
    "                    pred_num = extract_last_number(str(predicted))\n",
    "                    exp_num = extract_last_number(str(expected))\n",
    "                    is_correct = (pred_num is not None and exp_num is not None and pred_num == exp_num)\n",
    "\n",
    "            if is_correct:\n",
    "                correct += 1\n",
    "\n",
    "            # Print detailed information about the example\n",
    "            print(\"\\nPrompt:\")\n",
    "            print(full_prompt)\n",
    "            print(\"\\nExpected Answer:\")\n",
    "            print(expected)\n",
    "            print(\"\\nPredicted Answer:\")\n",
    "            print(predicted)\n",
    "            print(\"\\nFull Generated Response:\")\n",
    "            print(response)\n",
    "            print(\"\\nCorrect:\", \"✓\" if is_correct else \"✗\")\n",
    "            print(\"-\"*50)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"\\nFailed to parse model output for example:\")\n",
    "            print(full_prompt)\n",
    "            print(\"Error:\", e)\n",
    "            print(\"-\"*50)\n",
    "\n",
    "    # Calculate the overall accuracy\n",
    "    accuracy = (correct / total) * 100\n",
    "    print(f\"\\nAccuracy: {correct}/{total} ({accuracy:.2f}%)\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    model.train() # Set the model back to training mode\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "correctness_reward:\n",
    "This function assigns rewards based on whether the generated answer is correct. It compares the extracted answer from the model output with the expected answer, \n",
    "using both exact string matching and numeric equivalence checks. A exact match earns a higher reward (2.0), while a match based on numeric equivalence receives a smaller reward (1.5).\n",
    "       1. Extracts the content from each completion.\n",
    "       2. Extracts the answer portion from each response using extract_answer_from_model_output.\n",
    "       3. Assigns rewards based on matching criteria:\n",
    "          - 2.0 points for an exact match\n",
    "          - 1.5 points for numeric equivalence (when values match but format differs)\n",
    "          - 0.0 points for incorrect answers\n",
    "       4. Tracks completion lengths for analysis.\n",
    "\n",
    "format_reward:\n",
    "This function encourages the model to adhere to the desired XML-like output format. It provides a small reward for the presence of the <reasoning>, </reasoning>, <answer>, and </answer> tags in the generated text.\n",
    "We use a relatively value of 0.05 for each of the four pieces because the model is already capable of using these tags from previous supervised finetuning step, so we give this small reward \n",
    "so that it doesn't forget to do that because of the RL updates.\n",
    "       1. Extracts the content from each completion.\n",
    "       2. Evaluates format compliance by checking for required XML tags:\n",
    "          - 0.2 points for each tag present (<reasoning>, </reasoning>, <answer>, </answer>)\n",
    "          - Maximum score of 0.8 for perfect format compliance\n",
    "       3. Stores and returns the format compliance scores.\n",
    "\n",
    "combined_reward:\n",
    "       1. Calculates separate rewards for correctness and format compliance.\n",
    "       2. Combines the rewards with the following weights:\n",
    "          - Correctness score range: 0.0 to 2.0\n",
    "          - Format score range: 0.0 to 0.8\n",
    "          - Total possible range: 0.0 to 2.8\n",
    "       3. Returns the combined reward for each example.\n",
    "\"\"\"\n",
    "\n",
    "def correctness_reward(prompts, completions, answer, **kwargs):\n",
    "    responses = [completion[0]['content'] for completion in completions]\n",
    "    extracted = [extract_answer_from_model_output(response) for response in responses]\n",
    "    rewards = []\n",
    "\n",
    "    for r, a in zip(extracted, answer):\n",
    "        if r == a:\n",
    "            rewards.append(2.0)\n",
    "        else:\n",
    "            r_num = extract_single_number(str(r))\n",
    "            a_num = extract_single_number(str(a))\n",
    "            if r_num is not None and a_num is not None and r_num == a_num:\n",
    "                rewards.append(1.5)\n",
    "            else:\n",
    "                rewards.append(0.0)\n",
    "    completion_lengths = [len(response.split()) for response in responses]\n",
    "    return rewards\n",
    "\n",
    "def format_reward(completions, **kwargs):\n",
    "    responses = [completion[0]['content'] for completion in completions]\n",
    "    rewards = []\n",
    "    format_scores = []\n",
    "    for response in responses:\n",
    "        score = 0.0\n",
    "        if \"<reasoning>\" in response:\n",
    "            score += 0.2\n",
    "        if \"</reasoning>\" in response:\n",
    "            score += 0.2\n",
    "        if \"<answer>\" in response:\n",
    "            score += 0.2\n",
    "        if \"</answer>\" in response:\n",
    "            score += 0.2\n",
    "        rewards.append(score)\n",
    "        format_scores.append(score)\n",
    "    return rewards\n",
    "    \n",
    "def combined_reward(prompts, completions, answer):\n",
    "    correctness_rewards = correctness_reward(prompts, completions, answer)\n",
    "    format_rewards = format_reward(completions)\n",
    "    \n",
    "    combined_rewards = []\n",
    "    for c_reward, f_reward in zip(correctness_rewards, format_rewards):\n",
    "        combined_rewards.append(c_reward + f_reward)\n",
    "    \n",
    "    return combined_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: GRPO Code\n",
    "# we implement all the building blocks of the GRPO algorithm from scratch. The implementation assumes that the machine running the code has at least 2 GPUs. \n",
    "# We use PyTorch's DataParallel API to distribute the policy model across the GPU cores, one copy of the model per GPU core. The batch is split between the GPU cores.\n",
    "\n",
    "def selective_log_softmax(logits, input_ids):\n",
    "    \"\"\"\n",
    "    Computes log probabilities for specific tokens in the vocabulary.\n",
    "\n",
    "    Args:\n",
    "        logits (torch.Tensor): The raw logits output from the model.\n",
    "        input_ids (torch.Tensor): The token IDs for which we want the log probabilities.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Log probabilities of the selected tokens.\n",
    "\n",
    "    Explanation:\n",
    "        1. Applies log softmax to convert logits to log probabilities over the vocabulary.\n",
    "        2. Uses gather to extract only the log probabilities corresponding to the input_ids.\n",
    "        3. Removes the extra dimension to match the original shape of input_ids.\n",
    "    \"\"\"\n",
    "    log_probs = nn.functional.log_softmax(logits, dim=-1)\n",
    "    return log_probs.gather(dim=-1, index=input_ids.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "def compute_log_probs(model, input_ids, attention_mask, logits_to_keep):\n",
    "    \"\"\"\n",
    "    Computes the log probabilities for a batch of tokens.\n",
    "\n",
    "    Args:\n",
    "        model: The language model.\n",
    "        input_ids (torch.Tensor): Token IDs for input sequences.\n",
    "        attention_mask (torch.Tensor): Attention mask for input sequences.\n",
    "        logits_to_keep (int): Number of tokens to keep from the end of the sequence.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Log probabilities of the selected tokens.\n",
    "\n",
    "    Explanation:\n",
    "        1. Gets logits from the model for the input sequence.\n",
    "        2. Selects logits for all tokens except the last one (as we predict next tokens).\n",
    "        3. Selects only the last 'logits_to_keep' tokens from both logits and input_ids.\n",
    "        4. Computes log probabilities for these tokens using selective_log_softmax.\n",
    "    \"\"\"\n",
    "    logits = model(input_ids=input_ids, attention_mask=attention_mask).logits[:, :-1, :]\n",
    "    input_ids = input_ids[:, -logits_to_keep:]\n",
    "    logits = logits[:, -logits_to_keep:, :]\n",
    "    return selective_log_softmax(logits, input_ids)\n",
    "\n",
    "def create_completion_mask(completion_ids, eos_token_id):\n",
    "    \"\"\"\n",
    "    Creates a mask for completion tokens that excludes tokens after the EOS token.\n",
    "\n",
    "    Args:\n",
    "        completion_ids (torch.Tensor): Token IDs of the generated completions.\n",
    "        eos_token_id (int): The ID of the end-of-sequence token.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A binary mask with 1s for valid tokens and 0s after the EOS token.\n",
    "\n",
    "    Explanation:\n",
    "        1. Identifies positions where EOS tokens occur in each sequence.\n",
    "        2. Finds the index of the first EOS token in each sequence.\n",
    "        3. Creates a mask where positions before and including the first EOS are 1, others are 0.\n",
    "        4. If no EOS token is found in a sequence, all positions are set to 1.\n",
    "    \"\"\"\n",
    "    is_eos = completion_ids == eos_token_id\n",
    "    eos_idx = torch.full((is_eos.size(0),), is_eos.size(1), dtype=torch.long, device=completion_ids.device)\n",
    "    mask_exists = is_eos.any(dim=1)\n",
    "    eos_idx[mask_exists] = is_eos.int().argmax(dim=1)[mask_exists]\n",
    "    sequence_indices = torch.arange(is_eos.size(1), device=completion_ids.device).expand(is_eos.size(0), -1)\n",
    "    return (sequence_indices <= eos_idx.unsqueeze(1)).int()\n",
    "\n",
    "def generate_completions(model, tokenizer, prompts, num_generations=4, max_completion_length=32):\n",
    "    \"\"\"\n",
    "    Generates multiple completions for each prompt.\n",
    "\n",
    "    Args:\n",
    "        model: The language model.\n",
    "        tokenizer: The tokenizer for encoding and decoding text.\n",
    "        prompts (list): List of text prompts.\n",
    "        num_generations (int): Number of completions to generate per prompt.\n",
    "        max_completion_length (int): Maximum number of tokens to generate.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Containing prompt IDs, prompt mask, completion IDs, and completion mask.\n",
    "\n",
    "    Explanation:\n",
    "        1. Encodes the prompts and moves them to the appropriate device.\n",
    "        2. Repeats each prompt num_generations times to generate multiple completions.\n",
    "        3. Generates completions using the model with specified parameters.\n",
    "        4. Extracts the completion IDs (excluding the prompt tokens).\n",
    "        5. Creates a mask for the completions using create_completion_mask.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, padding_side=\"left\")\n",
    "    prompt_ids = inputs[\"input_ids\"].to(device)\n",
    "    prompt_mask = inputs[\"attention_mask\"].to(device)\n",
    "    print(f\"Input batch size: {prompt_ids.size(0)}, Device before model: {prompt_ids.device}\")\n",
    "    prompt_length = prompt_ids.size(1)\n",
    "    prompt_ids = prompt_ids.repeat_interleave(num_generations, dim=0)\n",
    "    prompt_mask = prompt_mask.repeat_interleave(num_generations, dim=0)\n",
    "    outputs = model.generate(\n",
    "        prompt_ids,\n",
    "        attention_mask=prompt_mask,\n",
    "        max_new_tokens=max_completion_length,\n",
    "        do_sample=True,\n",
    "        temperature=1.0,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        early_stopping=False\n",
    "    )\n",
    "    print(f\"Output batch size: {outputs.size(0)}, Device after model: {outputs.device}\")\n",
    "    completion_ids = outputs[:, prompt_length:]\n",
    "    completion_mask = create_completion_mask(completion_ids, tokenizer.eos_token_id)\n",
    "    return prompt_ids, prompt_mask, completion_ids, completion_mask\n",
    "\n",
    "def generate_rollout_data(model, ref_model, tokenizer, batch_samples, num_generations, max_completion_length):\n",
    "    \"\"\"\n",
    "    Generates data for GRPO rollouts including completions and log probabilities.\n",
    "\n",
    "    Args:\n",
    "        model: The policy model being trained.\n",
    "        ref_model: The reference model for KL divergence calculation.\n",
    "        tokenizer: The tokenizer for encoding and decoding text.\n",
    "        batch_samples (list): Batch of training samples.\n",
    "        num_generations (int): Number of completions to generate per sample.\n",
    "        max_completion_length (int): Maximum completion length.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing all data needed for GRPO updates.\n",
    "\n",
    "    Explanation:\n",
    "        1. Extracts prompts and expected answers from the batch samples.\n",
    "        2. Generates completions using the current policy model.\n",
    "        3. Combines prompt and completion tokens.\n",
    "        4. Computes log probabilities from both the policy model and reference model.\n",
    "        5. Formats completions for reward calculation.\n",
    "        6. Repeats prompts and answers to match the number of generated completions.\n",
    "        7. Returns all data needed for GRPO loss calculation.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    prompts = [sample[\"prompt\"] if isinstance(sample, dict) else sample[0] for sample in batch_samples]\n",
    "    answers = [sample[\"answer\"] if isinstance(sample, dict) else sample[1] for sample in batch_samples]\n",
    "    with torch.no_grad():\n",
    "        prompt_ids, prompt_mask, completion_ids, completion_mask = generate_completions(\n",
    "            model, tokenizer, prompts, num_generations, max_completion_length\n",
    "        )\n",
    "        input_ids = torch.cat([prompt_ids, completion_ids], dim=1)\n",
    "        attention_mask = torch.cat([prompt_mask, completion_mask], dim=1)\n",
    "        logits_to_keep = completion_ids.size(1)\n",
    "        old_log_probs = compute_log_probs(model, input_ids, attention_mask, logits_to_keep)\n",
    "        ref_log_probs = compute_log_probs(ref_model, input_ids, attention_mask, logits_to_keep)\n",
    "    formatted_completions = [[{'content': tokenizer.decode(ids, skip_special_tokens=True)}] for ids in completion_ids]\n",
    "    repeated_prompts = [p for p in prompts for _ in range(num_generations)]\n",
    "    repeated_answers = [a for a in answers for _ in range(num_generations)]\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"completion_mask\": completion_mask,\n",
    "        \"old_log_probs\": old_log_probs,\n",
    "        \"ref_log_probs\": ref_log_probs,\n",
    "        \"formatted_completions\": formatted_completions,\n",
    "        \"repeated_prompts\": repeated_prompts,\n",
    "        \"repeated_answers\": repeated_answers,\n",
    "        \"logits_to_keep\": logits_to_keep,\n",
    "        \"batch_size\": len(prompts),\n",
    "        \"num_generations\": num_generations\n",
    "    }\n",
    "\n",
    "def grpo_loss(model, ref_model, rollout_data, tokenizer, reward_function, beta=0.01, epsilon=0.2):\n",
    "    \"\"\"\n",
    "    Computes the GRPO loss for updating the policy model.\n",
    "\n",
    "    Args:\n",
    "        model: The policy model being trained.\n",
    "        ref_model: The reference model for KL divergence calculation.\n",
    "        rollout_data (dict): Data generated by generate_rollout_data.\n",
    "        tokenizer: The tokenizer for encoding and decoding text.\n",
    "        reward_function: Function that calculates rewards for completions.\n",
    "        beta (float): KL penalty coefficient.\n",
    "        epsilon (float): Clipping parameter for PPO.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The GRPO loss to be minimized.\n",
    "\n",
    "    Explanation:\n",
    "        1. Computes current token log probabilities using the policy model.\n",
    "        2. Calculates the probability ratio between current and old policies.\n",
    "        3. Computes rewards using the provided reward_function.\n",
    "        4. Calculates advantages by standardizing rewards within each prompt.\n",
    "        5. Computes the PPO surrogate objective with clipping.\n",
    "        6. Calculates the KL divergence between reference and policy models.\n",
    "        7. Combines surrogate loss and KL penalty.\n",
    "        8. Averages the loss across all tokens and batches.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    input_ids = rollout_data[\"input_ids\"]\n",
    "    attention_mask = rollout_data[\"attention_mask\"]\n",
    "    completion_mask = rollout_data[\"completion_mask\"]\n",
    "    logits_to_keep = rollout_data[\"logits_to_keep\"]\n",
    "    old_log_probs = rollout_data[\"old_log_probs\"]\n",
    "    ref_log_probs = rollout_data[\"ref_log_probs\"]\n",
    "    token_log_probs = compute_log_probs(model, input_ids, attention_mask, logits_to_keep)\n",
    "    ratio = torch.exp(token_log_probs - old_log_probs)\n",
    "    rewards = torch.tensor(\n",
    "        reward_function(prompts=rollout_data[\"repeated_prompts\"], completions=rollout_data[\"formatted_completions\"], answer=rollout_data[\"repeated_answers\"]),\n",
    "        dtype=torch.float32,\n",
    "        device=device\n",
    "    )\n",
    "    #print(f\"Rewards: {rewards}\")  # Debug rewards\n",
    "    batch_size = rollout_data[\"batch_size\"]\n",
    "    num_generations = rollout_data[\"num_generations\"]\n",
    "    rewards = rewards.view(batch_size, num_generations)\n",
    "    avg_reward = rewards.mean().item()\n",
    "    print(\"Average Reward:\", avg_reward)\n",
    "    mean_rewards = rewards.mean(dim=1).repeat_interleave(num_generations)\n",
    "    std_rewards = rewards.std(dim=1).repeat_interleave(num_generations)\n",
    "    advantages = ((rewards.view(-1) - mean_rewards) / (std_rewards + 1e-4)).unsqueeze(1)\n",
    "    surr1 = ratio * advantages\n",
    "    surr2 = torch.clamp(ratio, 1 - epsilon, 1 + epsilon) * advantages\n",
    "    surrogate_loss = torch.min(surr1, surr2)\n",
    "    kl = torch.exp(ref_log_probs - token_log_probs) - (ref_log_probs - token_log_probs) - 1\n",
    "    per_token_loss = surrogate_loss - beta * kl\n",
    "    loss = -((per_token_loss * completion_mask).sum(dim=1) / completion_mask.sum(dim=1)).mean()\n",
    "    return loss, avg_reward\n",
    "\n",
    "def train_with_grpo(model, tokenizer, train_data, num_iterations=1, num_steps=500, batch_size=4,\n",
    "                              num_generations=4, max_completion_length=128, beta=0.1,\n",
    "                              learning_rate=5e-6, mu=3, epsilon=0.2, reward_function=None, device_ids=None):\n",
    "    \"\"\"\n",
    "    This function is your original working code (train_with_grpo_static)\n",
    "    with an added outer loop for iterative GRPO updates per the pseudocode.\n",
    "\n",
    "    Args:\n",
    "        model: The language model to train.\n",
    "        tokenizer: The tokenizer for encoding and decoding text.\n",
    "        train_data (list): Training dataset.\n",
    "        num_iterations (int): Number of outer iterations (reference model updates).\n",
    "        num_steps (int): Number of batch updates per iteration.\n",
    "        batch_size (int): Number of prompts per batch.\n",
    "        num_generations (int): Number of completions per prompt.\n",
    "        max_completion_length (int): Maximum token length for completions.\n",
    "        beta (float): KL penalty coefficient.\n",
    "        learning_rate (float): Learning rate for optimizer.\n",
    "        mu (int): Number of policy updates per batch.\n",
    "        epsilon (float): PPO clipping parameter.\n",
    "        reward_function: Function that calculates rewards for completions.\n",
    "        device_ids (list): List of GPU device IDs for DataParallel.\n",
    "\n",
    "    Returns:\n",
    "        The trained model.\n",
    "\n",
    "    Explanation:\n",
    "        1. For each outer iteration:\n",
    "           - Creates a reference model as a deep copy of the current policy model.\n",
    "           - Reinitializes the optimizer for the policy model.\n",
    "           - For each training step:\n",
    "             a. Samples a batch of examples from the training data.\n",
    "             b. Generates rollout data including completions and log probabilities.\n",
    "             c. For mu iterations:\n",
    "                i. Computes the GRPO loss.\n",
    "                ii. Updates the policy model using gradient descent.\n",
    "           - Monitors GPU memory usage and prints progress information.\n",
    "    \"\"\"\n",
    "    assert device_ids is not None and len(device_ids) > 1, \"This code needs at least 2 GPU cores to run!\"\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Wrap model with DataParallel if multiple GPUs are available.\n",
    "\n",
    "    model = nn.DataParallel(model, device_ids=device_ids)\n",
    "    print(f\"Model wrapped with DataParallel across GPUs: {device_ids}\")\n",
    "\n",
    "    # Outer loop: iterative GRPO updates.\n",
    "    for iteration in range(num_iterations):\n",
    "        print(f\"\\nIteration {iteration+1}/{num_iterations}\")\n",
    "\n",
    "        # Create a reference model (deep copy) and set it to eval mode.\n",
    "        ref_model = copy.deepcopy(model.module)\n",
    "        ref_model.eval()\n",
    "        for param in ref_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        print(\"Reference model created.\")\n",
    "\n",
    "        # Reinitialize the optimizer for this iteration.\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "        model.train()\n",
    "\n",
    "        # Inner loop: your original training steps.\n",
    "        for step in range(num_steps):\n",
    "            batch_samples = random.sample(train_data, batch_size)\n",
    "            with torch.no_grad():\n",
    "                rollout_data = generate_rollout_data(\n",
    "                    model.module,\n",
    "                    ref_model,\n",
    "                    tokenizer,\n",
    "                    batch_samples,\n",
    "                    num_generations,\n",
    "                    max_completion_length\n",
    "                )\n",
    "            for grpo_iter in range(mu):\n",
    "                loss, avg_reward = grpo_loss(\n",
    "                    model.module,\n",
    "                    ref_model,\n",
    "                    rollout_data,\n",
    "                    tokenizer,\n",
    "                    reward_function,\n",
    "                    beta=beta,\n",
    "                    epsilon=epsilon\n",
    "                )\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.1)\n",
    "                optimizer.step()\n",
    "                # Log to wandb\n",
    "                wandb.log({\n",
    "                    \"loss\": loss.item(),\n",
    "                    \"average_reward\": avg_reward,\n",
    "                    \"iteration\": iteration + 1,\n",
    "                    \"step\": step + 1,\n",
    "                    \"grpo_iter\": grpo_iter + 1\n",
    "                })\n",
    "                print(f\"Iteration {iteration+1}/{num_iterations}, Step {step+1}/{num_steps}, \"\n",
    "                      f\"GRPO iter {grpo_iter+1}/{mu}, loss: {loss.item():.4f}\")\n",
    "                #for i in range(torch.cuda.device_count()):\n",
    "                #    print(f\"GPU {i} Usage: {torch.cuda.memory_allocated(i) / 1024**2:.2f} MiB, \"\n",
    "                #          f\"Utilization: {torch.cuda.utilization(i)}%\")\n",
    "                # Uncomment to see the GPU utilization stats\n",
    "    return model.module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TODO:\n",
    "- Many transformer models store past activations (attention KV cache) to speed up inference.\n",
    "  This reduces memory usage at the cost of recomputing previous activations.\n",
    "- If an input does not require gradients, it won’t contribute to the learning process.\n",
    "- Enables gradient checkpointing to save memory.\n",
    "  Normally, activations are stored for all layers during the forward pass to compute gradients in the backward pass.\n",
    "  With gradient checkpointing:\n",
    "   - Only some activations are saved.\n",
    "   - Other activations are recomputed on-demand during backpropagation.\n",
    "   - This significantly reduces memory consumption but slightly increases computation time.\n",
    "\"\"\"\n",
    "\n",
    "# optimize model memory\n",
    "def optimize_model_memory(model):\n",
    "    model.train() # Set the model to training mode\n",
    "    model.config.use_cache = False # Disable KV caching\n",
    "\n",
    "    # ensure that input requires gradients\n",
    "    if hasattr(model, \"enable_input_require_grads\"):\n",
    "        model.enable_input_require_grads() # Enable input gradients\n",
    "    else:\n",
    "        def make_input_requires_grad(module, input, output):\n",
    "            output.requires_grad = True # Set input gradients to True\n",
    "        model.get_input_embeddings().register_forward_hook(make_input_requires_grad) # Register forward hook\n",
    "\n",
    "    model.gradient_checkpointing_enable() # Enable gradient checkpointing\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The device is determined (GPU if available, otherwise CPU).\n",
    "The pre-trained Qwen2.5-1.5B-Instruct model and tokenizer are loaded. The tokenizer's pad token is set to the eos_token.\n",
    "A small subset of the dataset is reserved for evaluation to provide a baseline.\n",
    "The model is optimized for memory efficiency by enabling gradient checkpointing and disabling KV caching.\n",
    "Step 1: The model is evaluated before fine-tuning to establish a baseline accuracy.\n",
    "Step 2: Reinforcement learning fine-tuning is performed using the train_with_grpo function with our defined reward functions (format_reward and correctness_reward, combined into combined_reward). The model is trained using a multi-GPU.\n",
    "Step 3: The final, fine-tuned model and tokenizer are saved to disk.\n",
    "\"\"\"\n",
    "\n",
    "# Model Execution\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using primary device: {device}\")\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "output_dir = \"ms_math_solver_model\"\n",
    "\n",
    "print(\"Downloading Model...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16, # Use bfloat16 for memory efficiency\n",
    "    device_map=\"auto\" # Automatically distribute model across available GPUs\n",
    ")\n",
    "print(\"Model Downloaded.\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side=\"left\") # Load tokenizer, set padding side to left\n",
    "tokenizer.pad_token = tokenizer.eos_token # Set the pad token to the EOS token\n",
    "model.config.pad_token_id = tokenizer.eos_token_id # Set the model's token ID to the EOS token ID\n",
    "model.config.eos_token_id = tokenizer.eos_token_id # Set the model's end-of-sequence token ID\n",
    "\n",
    "num_gpus = torch.cuda.device_count()\n",
    "print(f\"Number of GPUs available: {num_gpus}\")\n",
    "device_ids = list(range(num_gpus)) if num_gpus > 1 else None\n",
    "\n",
    "all_data = prepare_dataset(\"train\") # Load the training dataset\n",
    "random.shuffle(all_data) # Shuffle the dataset for randomness\n",
    "size_of_eval_set = 30 # Size of the evaluation set\n",
    "eval_data = all_data[:size_of_eval_set] # Reserve a subset for evaluation\n",
    "train_data = all_data[size_of_eval_set:] # Use the remaining data for training\n",
    "\n",
    "print(\"/nInitial Evaluation before the finetuning...\")  \n",
    "pre_grpo_accuracy = evaluate_model(model, tokenizer, eval_data, device) # Evaluate the model before fine-tuning\n",
    "print(f\"Initial Evaluation Accuracy before GRPO: {pre_grpo_accuracy:.2f}%\")\n",
    "\n",
    "model = optimize_model_memory(model) # Optimize the model for memory efficiency\n",
    "\n",
    "print(\"\\nFine-tuning the model using GRPO...\")\n",
    "training_config = {\n",
    "    'num_iterations': 1,\n",
    "    'num_steps': 500,\n",
    "    'batch_size': 7,\n",
    "    'num_generations': 12,\n",
    "    'max_completion_length': 400,\n",
    "    'beta': 0.4, # KL penalty coefficient\n",
    "    'learning_rate': 5e-6, \n",
    "    'mu': 1, # Number of policy updates per batch\n",
    "    'epsilon': 0.1 # PPO clipping parameter\n",
    "}\n",
    "\n",
    "# initialize wandb\n",
    "wandb.init(project=os.environ[\"WANDB_PROJECT\"], reinit=True)\n",
    "print(\"Wandb initialized.\")\n",
    "\n",
    "model = train_with_grpo(\n",
    "    model=model, \n",
    "    tokenizer=tokenizer,\n",
    "    train_data=train_data,\n",
    "    reward_function=combined_reward,\n",
    "    device_ids=device_ids,\n",
    "    **training_config \n",
    ")\n",
    "\n",
    "wandb.finish()\n",
    "print(\"Training completed.\")\n",
    "\n",
    "print(\"\\nEvaluation after the finetuning...\")\n",
    "post_grpo_accuracy = evaluate_model(model, tokenizer, eval_data, device) # Evaluate the model after fine-tuning\n",
    "print(f\"Final Evaluation Accuracy after GRPO: {post_grpo_accuracy:.2f}%\")\n",
    "\n",
    "print(\"\\nSaving the final model and tokenizer...\")\n",
    "model.save_pretrained(\"ms_math_solver_model\") # Save the final model\n",
    "tokenizer.save_pretrained(\"ms_math_solver_model\") # Save the final tokenizer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "grpo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
